{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDERS = {\n",
    "    \"BAAI/bge-large-en\": {\"type\": \"huggingface\", \"model\": \"BAAI/bge-large-en\"},\n",
    "    \"all-MiniLM-L6-v2\": {\"type\": \"sentence-transformers\", \"model\": \"all-MiniLM-L6-v2\"},\n",
    "    \"nomic-embed-text-v2-moe\": {\"type\": \"huggingface\", \"model\": \"nomic-ai/nomic-embed-text-v2-moe\", \"trust_remote_code\": True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(use_gpu=True):\n",
    "    \"\"\"Determine the best device to use\"\"\"\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif use_gpu and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")  # For Apple Silicon\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(data_dir):\n",
    "    \"\"\"Load all text files from the specified directory\"\"\"\n",
    "    documents = []\n",
    "    filenames = []\n",
    "\n",
    "    if not data_dir or not os.path.exists(data_dir):\n",
    "        print(f\"Directory {data_dir} does not exist.\")\n",
    "        return documents, filenames\n",
    "\n",
    "    txt_files = [f for f in os.listdir(data_dir) if f.endswith('.txt')]\n",
    "    if not txt_files:\n",
    "        print(f\"No .txt files found in {data_dir}\")\n",
    "        return documents, filenames\n",
    "\n",
    "    for filename in txt_files:\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                content = file.read().strip()\n",
    "                if content:  # Only add non-empty files\n",
    "                    documents.append(content)\n",
    "                    filenames.append(filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {str(e)}\")\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"Mean pooling for Hugging Face models\"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_huggingface(texts, model_name, device, trust_remote_code=False):\n",
    "    \"\"\"Get embeddings using Hugging Face models with GPU support\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=trust_remote_code,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "    ).to(device)\n",
    "\n",
    "    batch_size = 16 if device.type == \"cuda\" else 8\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        batch_embeddings = mean_pooling(outputs, inputs['attention_mask']).cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.vstack(embeddings) if embeddings else np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_sentence_transformers(texts, model_name, device):\n",
    "    \"\"\"Get embeddings using SentenceTransformers with GPU support\"\"\"\n",
    "    if not texts:\n",
    "        return np.array([])\n",
    "\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    st_device = \"cuda\" if device.type == \"cuda\" else \"cpu\"\n",
    "    model = SentenceTransformer(model_name, device=st_device)\n",
    "\n",
    "    batch_size = 64 if device.type == \"cuda\" else 32\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(\n",
    "            batch_texts,\n",
    "            batch_size=batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            device=st_device\n",
    "        )\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.vstack(embeddings) if embeddings else np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(embedder_name, embedder_config, texts, device):\n",
    "    \"\"\"Get embeddings based on embedder type\"\"\"\n",
    "    print(f\"Generating embeddings using {embedder_name} on {device}...\")\n",
    "\n",
    "    if not texts:\n",
    "        print(\"No documents to process\")\n",
    "        return np.array([])\n",
    "\n",
    "    if embedder_config[\"type\"] == \"huggingface\":\n",
    "        trust_remote_code = embedder_config.get(\"trust_remote_code\", False)\n",
    "        return get_embeddings_huggingface(texts, embedder_config[\"model\"], device, trust_remote_code)\n",
    "    elif embedder_config[\"type\"] == \"sentence-transformers\":\n",
    "        return get_embeddings_sentence_transformers(texts, embedder_config[\"model\"], device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown embedder type: {embedder_config['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(embeddings, max_clusters=10):\n",
    "    \"\"\"Find optimal number of clusters using elbow method and silhouette analysis for KMeans\"\"\"\n",
    "    if len(embeddings) <= 2:\n",
    "        return 1\n",
    "\n",
    "    max_clusters = min(max_clusters, len(embeddings) - 1)\n",
    "    wcss = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "    k_range = range(1, max_clusters + 1)\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "        if k > 1:\n",
    "            try:\n",
    "                silhouette_scores.append(silhouette_score(embeddings, clusters))\n",
    "            except:\n",
    "                silhouette_scores.append(0)\n",
    "        else:\n",
    "            silhouette_scores.append(0)\n",
    "\n",
    "    # Elbow point\n",
    "    if len(wcss) > 2:\n",
    "        deltas = np.diff(wcss)\n",
    "        derivatives = np.diff(deltas)\n",
    "        if len(derivatives) > 0:\n",
    "            elbow_point = np.argmin(derivatives) + 2\n",
    "        else:\n",
    "            elbow_point = 2\n",
    "    else:\n",
    "        elbow_point = 2\n",
    "\n",
    "    # Silhouette\n",
    "    if len(silhouette_scores) > 1:\n",
    "        optimal_k_silhouette = np.argmax(silhouette_scores[1:]) + 2\n",
    "    else:\n",
    "        optimal_k_silhouette = 1\n",
    "\n",
    "    optimal_k = max(1, min(elbow_point, optimal_k_silhouette))\n",
    "\n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.plot(k_range, wcss, 'bo-')\n",
    "    ax1.set_xlabel('Number of clusters')\n",
    "    ax1.set_ylabel('WCSS')\n",
    "    ax1.set_title('Elbow Method')\n",
    "    ax1.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal k: {optimal_k}')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(k_range, silhouette_scores, 'bo-')\n",
    "    ax2.set_xlabel('Number of clusters')\n",
    "    ax2.set_ylabel('Silhouette Score')\n",
    "    ax2.set_title('Silhouette Analysis')\n",
    "    ax2.axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal k: {optimal_k}')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return optimal_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(embeddings, clustering_method, clustering_params):\n",
    "    \"\"\"Perform clustering based on the selected method\"\"\"\n",
    "    if clustering_method == 'kmeans':\n",
    "        n_clusters = clustering_params.get('n_clusters', 8)\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    elif clustering_method == 'dbscan':\n",
    "        eps = clustering_params.get('eps', 0.5)\n",
    "        min_samples = clustering_params.get('min_samples', 5)\n",
    "        clusterer = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    elif clustering_method == 'agg':\n",
    "        n_clusters = clustering_params.get('n_clusters', 8)\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    elif clustering_method == 'gmm':\n",
    "        n_components = clustering_params.get('n_components', clustering_params.get('n_clusters', 8))\n",
    "        clusterer = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown clustering method: {clustering_method}\")\n",
    "\n",
    "    clusters = clusterer.fit_predict(embeddings)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dim_reduction(embeddings, dim_red_method, n_components=2):\n",
    "    \"\"\"Perform dimensionality reduction for visualization\"\"\"\n",
    "    if dim_red_method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        reduced = reducer.fit_transform(embeddings)\n",
    "        variance_ratio = reducer.explained_variance_ratio_\n",
    "    elif dim_red_method == 'tsne':\n",
    "        reducer = TSNE(n_components=n_components, random_state=42)\n",
    "        reduced = reducer.fit_transform(embeddings)\n",
    "        variance_ratio = [None, None]\n",
    "    elif dim_red_method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components, random_state=42)\n",
    "        reduced = reducer.fit_transform(embeddings)\n",
    "        variance_ratio = [None, None]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dim reduction method: {dim_red_method}\")\n",
    "\n",
    "    return reduced, variance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_plot(reduced_embeddings, clusters, embedder_name, variance_ratio, num_clusters, filenames, documents, dim_red_method):\n",
    "    \"\"\"Create an interactive plot using Plotly\"\"\"\n",
    "    plot_df = pd.DataFrame({\n",
    "        'x': reduced_embeddings[:, 0],\n",
    "        'y': reduced_embeddings[:, 1],\n",
    "        'cluster': [f'Cluster {c}' for c in clusters],\n",
    "        'filename': filenames,\n",
    "        'content': [doc[:100] + '...' if len(doc) > 100 else doc for doc in documents]\n",
    "    })\n",
    "\n",
    "    if variance_ratio[0] is not None:\n",
    "        title = f'{embedder_name} - {num_clusters} Clusters (PCA: {variance_ratio[0]*100:.1f}% + {variance_ratio[1]*100:.1f}% variance explained)'\n",
    "        labels = {'x': f'PCA Component 1 ({variance_ratio[0]*100:.1f}%)', 'y': f'PCA Component 2 ({variance_ratio[1]*100:.1f}%)'}\n",
    "    else:\n",
    "        title = f'{embedder_name} - {num_clusters} Clusters ({dim_red_method.upper()})'\n",
    "        labels = {'x': f'{dim_red_method.upper()} Component 1', 'y': f'{dim_red_method.upper()} Component 2'}\n",
    "\n",
    "    fig = px.scatter(\n",
    "        plot_df, x='x', y='y', color='cluster',\n",
    "        hover_data=['filename', 'content'],\n",
    "        title=title,\n",
    "        labels=labels\n",
    "    )\n",
    "\n",
    "    fig.update_traces(marker=dict(size=12, line=dict(width=1, color='DarkSlateGrey')))\n",
    "    fig.update_layout(width=800, height=600, hovermode='closest')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_details(cluster_assignments, embedder_name):\n",
    "    \"\"\"Print detailed cluster information\"\"\"\n",
    "    print(f\"\\nCluster assignments for {embedder_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for cluster_id in sorted(cluster_assignments.keys()):\n",
    "        items = cluster_assignments[cluster_id]\n",
    "        print(f\"Cluster {cluster_id}:\")\n",
    "        print(f\"  Number of documents: {len(items)}\")\n",
    "        print(\"  Documents:\")\n",
    "        for item in items:\n",
    "            print(f\"    - {item['filename']} (Index: {item['index']})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_embedder(embedder_name, embedder_config, documents, filenames, device, clustering_method, dim_red_method, clustering_params):\n",
    "    \"\"\"Complete analysis for a single embedder\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing with {embedder_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    try:\n",
    "        embeddings = get_embeddings(embedder_name, embedder_config, documents, device)\n",
    "        if embeddings.size == 0:\n",
    "            print(\"No embeddings generated\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "        if len(documents) <= 1:\n",
    "            print(\"Not enough documents for clustering\")\n",
    "            return None\n",
    "\n",
    "        local_params = clustering_params.copy()\n",
    "        if clustering_method == 'kmeans' and local_params.get('n_clusters') is None:\n",
    "            optimal_clusters = find_optimal_clusters(embeddings, max_clusters=10)\n",
    "            local_params['n_clusters'] = optimal_clusters\n",
    "        else:\n",
    "            optimal_clusters = local_params.get('n_clusters')  # May be None for some methods\n",
    "\n",
    "        print(f\"Number of clusters: {optimal_clusters if optimal_clusters else 'Auto (e.g., DBSCAN)'}\")\n",
    "\n",
    "        clusters = perform_clustering(embeddings, clustering_method, local_params)\n",
    "        reduced_embeddings, variance_ratio = perform_dim_reduction(embeddings, dim_red_method)\n",
    "\n",
    "        # Calculate actual number of clusters (excluding noise for DBSCAN)\n",
    "        unique_clusters = set(clusters)\n",
    "        actual_num_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n",
    "\n",
    "        # Cluster assignments\n",
    "        cluster_assignments = {}\n",
    "        for i, cluster_id in enumerate(clusters):\n",
    "            if cluster_id not in cluster_assignments:\n",
    "                cluster_assignments[cluster_id] = []\n",
    "            cluster_assignments[cluster_id].append({\n",
    "                'index': i,\n",
    "                'filename': filenames[i],\n",
    "                'embedding': reduced_embeddings[i]\n",
    "            })\n",
    "\n",
    "        # Similarities\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        intra_cluster_similarities = []\n",
    "        inter_cluster_similarities = []\n",
    "\n",
    "        for cluster_id, items in cluster_assignments.items():\n",
    "            indices = [item['index'] for item in items]\n",
    "\n",
    "            if len(indices) > 1:\n",
    "                cluster_similarities = similarity_matrix[np.ix_(indices, indices)]\n",
    "                np.fill_diagonal(cluster_similarities, 0)\n",
    "                intra_sum = np.sum(cluster_similarities) / (len(indices) * (len(indices) - 1))\n",
    "                intra_cluster_similarities.append(intra_sum)\n",
    "\n",
    "            other_indices = [i for i in range(len(embeddings)) if i not in indices]\n",
    "            if other_indices:\n",
    "                inter_similarities = similarity_matrix[np.ix_(indices, other_indices)]\n",
    "                inter_sum = np.mean(inter_similarities)\n",
    "                inter_cluster_similarities.append(inter_sum)\n",
    "\n",
    "        avg_intra_similarity = np.mean(intra_cluster_similarities) if intra_cluster_similarities else 0\n",
    "        avg_inter_similarity = np.mean(inter_cluster_similarities) if inter_cluster_similarities else 0\n",
    "\n",
    "        create_interactive_plot(\n",
    "            reduced_embeddings, clusters, embedder_name,\n",
    "            variance_ratio, actual_num_clusters, filenames, documents, dim_red_method\n",
    "        )\n",
    "\n",
    "        result = {\n",
    "            'embeddings': embeddings,\n",
    "            'reduced_embeddings': reduced_embeddings,\n",
    "            'clusters': clusters,\n",
    "            'cluster_assignments': cluster_assignments,\n",
    "            'variance_ratio': variance_ratio,\n",
    "            'optimal_clusters': optimal_clusters,\n",
    "            'avg_intra_similarity': avg_intra_similarity,\n",
    "            'avg_inter_similarity': avg_inter_similarity\n",
    "        }\n",
    "\n",
    "        print_cluster_details(cluster_assignments, embedder_name)\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {embedder_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_embedders(embedders, documents, filenames, device, clustering_method, dim_red_method, clustering_params):\n",
    "    \"\"\"Run analysis for all configured embedders\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for embedder_name, embedder_config in embedders.items():\n",
    "        result = analyze_embedder(\n",
    "            embedder_name, embedder_config, documents, filenames, device,\n",
    "            clustering_method, dim_red_method, clustering_params\n",
    "        )\n",
    "        if result:\n",
    "            results[embedder_name] = result\n",
    "\n",
    "    if len(results) > 1:\n",
    "        compare_embedders(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embedders(results):\n",
    "    \"\"\"Compare results across different embedders\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON OF EMBEDDERS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    comparison_data = []\n",
    "\n",
    "    for embedder_name, result in results.items():\n",
    "        comparison_data.append({\n",
    "            'Embedder': embedder_name,\n",
    "            'Clusters': result['optimal_clusters'],\n",
    "            'Avg Intra-Cluster Similarity': result['avg_intra_similarity'],\n",
    "            'Avg Inter-Cluster Similarity': result['avg_inter_similarity'],\n",
    "            'Separation Score': result['avg_intra_similarity'] - result['avg_inter_similarity']\n",
    "        })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Number of Clusters', 'Intra-Cluster Similarity',\n",
    "                       'Inter-Cluster Similarity', 'Separation Score')\n",
    "    )\n",
    "\n",
    "    fig.add_trace(go.Bar(x=comparison_df['Embedder'], y=comparison_df['Clusters'], name='Clusters'), row=1, col=1)\n",
    "    fig.add_trace(go.Bar(x=comparison_df['Embedder'], y=comparison_df['Avg Intra-Cluster Similarity'], name='Intra-Cluster'), row=1, col=2)\n",
    "    fig.add_trace(go.Bar(x=comparison_df['Embedder'], y=comparison_df['Avg Inter-Cluster Similarity'], name='Inter-Cluster'), row=2, col=1)\n",
    "    fig.add_trace(go.Bar(x=comparison_df['Embedder'], y=comparison_df['Separation Score'], name='Separation'), row=2, col=2)\n",
    "\n",
    "    fig.update_layout(height=600, width=800, title_text=\"Embedder Comparison\", showlegend=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set your data directory path here\n",
    "    data_dir = \"../Data/TXT/Sorted/English\"  # CHANGE THIS TO YOUR ACTUAL PATH\n",
    "\n",
    "    # Configure clustering and dimensionality reduction\n",
    "    clustering_method = 'kmeans'  # Options: 'kmeans', 'dbscan', 'agg', 'gmm'\n",
    "    dim_red_method = 'pca'  # Options: 'pca', 'tsne', 'umap'\n",
    "    clustering_params = {'n_clusters': None}  # Set to None for auto (kmeans only); for dbscan: {'eps': 0.5, 'min_samples': 5}\n",
    "\n",
    "    use_gpu = True\n",
    "    device = get_device(use_gpu)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    documents, filenames = load_documents(data_dir)\n",
    "\n",
    "    if not documents:\n",
    "        print(\"No documents found. Please check the data directory path.\")\n",
    "        print(\"Current data directory:\", data_dir)\n",
    "        print(\"Make sure the directory exists and contains .txt files.\")\n",
    "    else:\n",
    "        analyze_all_embedders(EMBEDDERS, documents, filenames, device, clustering_method, dim_red_method, clustering_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
